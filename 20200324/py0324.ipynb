{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "py0324.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "el4KGrxF4K9X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "dcc6f060-ab4a-4476-d3b3-174777567b66"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9MMF-2w5yZY",
        "colab_type": "text"
      },
      "source": [
        "텐서플로 1.X 버전의 코드를 수정하지 않고 텐서플로 2.0에서 실행 가능\n",
        "\n",
        "텐서플로 1.X 파일에 아래의 코드 2줄을 추가하면 텐서플로 2.0에서 실행 가능\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "tf.disable_v2_behavior()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZUZQ3Ex5Efh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "b8c60641-c21d-44a4-c236-d6f98305b1c5"
      },
      "source": [
        "#converttf2.py\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "# 상수 선언\n",
        "hello = tf.constant('Hello, Tensorflow')\n",
        "# 세션 시작\n",
        "sess = tf.Session()\n",
        "# 세션 실행\n",
        "print(sess.run(hello))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "b'Hello, Tensorflow'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F6KQ6MY5EdJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "0c4f3db5-f0b8-4102-a16b-6e6873ab626a"
      },
      "source": [
        "#random01.py\n",
        "\n",
        "import tensorflow as tf\n",
        "sess = tf.Session()\n",
        "\n",
        "# 변수 선언\n",
        "a = tf.Variable(tf.random_uniform([1])) # 0 ~ 1 사이의 난수 1개 발생\n",
        "b = tf.Variable(tf.random_uniform([1], 0, 10)) # 0 ~ 10 사이의 난수 1개 발생\n",
        "\n",
        "# 모든 변수 초기화\n",
        "sess.run(tf.global_variables_initializer())\n",
        "print('a=', sess.run(a))\n",
        "print('b=', sess.run(b))\n",
        "\"\"\" 난수 상수 생성 \"\"\"\n",
        "\n",
        "# 정규분포 난수\n",
        "norm = tf.random_normal([2, 3], mean=-1, stddev=4)\n",
        "print(sess.run(norm))\n",
        "\n",
        "# 주어진 값들을 shuffle()함수로 무작위로 섞음\n",
        "c = tf.constant([[1, 2], [3, 4], [5, 6]])\n",
        "shuff = tf.random_shuffle(c)\n",
        "print(sess.run(shuff))\n",
        "\n",
        "# 균등분포 난수\n",
        "unif = tf.random_uniform([2,3], minval=0, maxval=3)\n",
        "print(sess.run(unif))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a= [0.52061677]\n",
            "b= [8.424455]\n",
            "[[ 1.8890152   1.0903366   1.2321899 ]\n",
            " [-0.76504445 -1.1396103   0.34278548]]\n",
            "[[1 2]\n",
            " [3 4]\n",
            " [5 6]]\n",
            "[[1.1791613  1.0675399  0.00538123]\n",
            " [0.76899254 1.1324522  2.8846931 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY51s8v55EnA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a8767186-8400-4dd4-f87a-64bb429721b1"
      },
      "source": [
        "#placeholder01.py\n",
        "\n",
        "import tensorflow as tf\n",
        "# 상수나 변수의 데이터 타입만 설정하고 실행단계에서 딕셔너리에 값을 대입해서 사용할 경우 placeholder 사용\n",
        "\n",
        "# 실수 자료형 1개를 가진 배열\n",
        "# 변수 선언\n",
        "node1 = tf.placeholder(tf.float32)\n",
        "node2 = tf.placeholder(tf.float32)\n",
        "\n",
        "# 변수 연산\n",
        "add = node1 + node2\n",
        "mul = node1 * node2\n",
        "\n",
        "# 세션 선언\n",
        "sess = tf.Session()\n",
        "\n",
        "# 출력\n",
        "print(sess.run(add, feed_dict={node1:3, node2:4.0}))\n",
        "print(sess.run(mul, feed_dict={node1:3, node2:4.0}))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.0\n",
            "12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o92frQWn5EpQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "78189cd2-59d7-4a0e-e5a0-20771db03dd0"
      },
      "source": [
        "#placeholder02.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# placeholder 정의\n",
        "a = tf.placeholder(tf.int32) # 정수 자료형 1개를 가진 배열\n",
        "b = tf.placeholder(tf.int32) # 정수 자료형 1개를 가진 배열\n",
        "\n",
        "add = tf.add(a, b)\n",
        "mul = tf.multiply(a, b)\n",
        "\n",
        "# Launch the default graph\n",
        "with tf.Session() as sess:\n",
        "  print(sess.run(add, feed_dict={a: 2, b: 3}))\n",
        "  print(sess.run(mul, feed_dict={a: 3, b: 5}))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "697NAcAY5Ero",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f579f7d5-24d8-44ec-8ec8-f9ec1ecf205a"
      },
      "source": [
        "#placeholder03.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# placeholder 정의\n",
        "a = tf.placeholder(tf.int32, [3]) # 정수 자료형 3개를 가진 배열\n",
        "\n",
        "# 배열을 모든 값을 2배하는 연산 정의하기\n",
        "b = tf.constant(2)\n",
        "op = a * b\n",
        "\n",
        "# 세션 시작하기\n",
        "sess = tf.Session()\n",
        "\n",
        "# placeholder에 값을 넣고 실행하기\n",
        "r1 = sess.run(op, feed_dict={ a: [1, 2, 3] })\n",
        "r2 = sess.run(op, feed_dict={ a: [10, 20, 10] })\n",
        "print(r1)\n",
        "print(r2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 4 6]\n",
            "[20 40 20]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_zk9A6j5Etx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5ac9e491-0bb8-4090-a130-43aa320f7686"
      },
      "source": [
        "#placeholder04.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# placeholder 정의\n",
        "a = tf.placeholder(tf.int32, [None]) # 배열의 크기를 None으로 지정\n",
        "\n",
        "# 배열의 모든 값을 10배하는 연산 정의하기\n",
        "b = tf.constant(10)\n",
        "op = a * b\n",
        "\n",
        "# 세션 시작하기\n",
        "sess = tf.Session()\n",
        "\n",
        "# placeholder에 값을 넣고 실행하기\n",
        "r1 = sess.run(op, feed_dict={a: [1,2,3,4,5]})\n",
        "r2 = sess.run(op, feed_dict={a: [10,20]})\n",
        "print(r1)\n",
        "print(r2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10 20 30 40 50]\n",
            "[100 200]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "588X5Yam5EwQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "66b56333-bb2d-48eb-9091-22d9fc878acb"
      },
      "source": [
        "#gradient_descent.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# 공부한 시간(x_data) : 2 4 6 8\n",
        "# 성적(y_data) : 81 93 91 97\n",
        "x_data = [2, 4, 6, 8] # 공부한 시간\n",
        "y_data = [81,93,91,97] # 성적\n",
        "\n",
        "# 기울기 a와 y 젃편 b의 값을 임의로 정함\n",
        "# 단, 기울기의 범위는 0 ~ 10 사이이며 y 절편은 0 ~ 100 사이에서 변하게 함\n",
        "# tf.random_normal([1]) : 난수 1개 발생\n",
        "a = tf.Variable(tf.random_uniform([1], 0, 10, dtype = tf.float64, seed = 0)) # 기울기\n",
        "b = tf.Variable(tf.random_uniform([1], 0, 100, dtype = tf.float64, seed = 0)) # 절편\n",
        "\n",
        "# y에 대한 일차 방정식 ( y = ax + b)\n",
        "y = a * x_data + b\n",
        "\n",
        "# 텐서플로 cost 구하기\n",
        "cost = tf.reduce_mean(tf.square( y - y_data ))\n",
        "\n",
        "# 학습률 값\n",
        "learning_rate = 0.1\n",
        "\n",
        "# cost 값을 최소로 하는 값 찾기 : 경사 하강법\n",
        "gradient_decent = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "# 텐서플로를 이용한 학습\n",
        "sess = tf.Session()\n",
        "\n",
        "# 변수 초기화\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# 2001번 실행(0번 째를 포함하므로)\n",
        "# 2000번 학습\n",
        "for step in range(2001):\n",
        "   sess.run(gradient_decent)\n",
        "   # 100번마다 결과 출력\n",
        "   if step % 100 == 0:\n",
        "      print(\"step: %.f, cost = %.4f, 기울기 a = %.4f, y 절편 b = %.4f\" % (step, sess.run(cost), sess.run(a), sess.run(b)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step: 0, cost = 29348.4665, 기울기 a = -28.2352, y 절편 b = 74.5831\n",
            "step: 100, cost = 1332833190343361000655520381662764125916604617849245023782993539225952697233047532021541465529928667911505892918858167159602024886095866802603032576.0000, 기울기 a = -6483838871800039086094962846546263238098387308584784871137117992860516352.0000, y 절편 b = -1086511987322886058995192342610079605140090437671876784394836575638257664.0000\n",
            "step: 200, cost = 60546656861665671273717878384283975907277699381866791502680558998625304314508583454185424633562676570601560598193588414780502103846090405320449112293075066601960711071567411405302442370698522950494985417273991346937109516972304884198175341805770754549203176021295390507802749001424772792320.0000, 기울기 a = -1381940670625259281720238551668281768144137432421629953806554981886639854845698404530537149701282188161364139775542138485990909743464418606317568.0000, y 절편 b = -231575018147625330118482180710286115748733283279484485717101242398186590923681581040770007086861949374116592656197014493958098531281732880039936.0000\n",
            "step: 300, cost = inf, 기울기 a = -294541560160332273312678747751395340052396584850847147916704422620301052958597026569983503820656684325498908717628462361544891589945151342707041057192894313754447138401826909005039525362399386282789221448698508083200.0000, y 절편 b = -49357015528385814696386114837575786630096486494395721054117725349788236575321786687928419625083326614260478157590868245866005877931302271112163586808434996003465334593143105759529070390720122646383863690796474564608.0000\n",
            "step: 400, cost = inf, 기울기 a = -62777463972046197284537766767906663391740384959769799869849539331011365199777779400719265992163758118019341127052749189544908147467311877469731088374984610141060749612897563635239460445329887809929969674392761970502845311645259239992263094219716150126603618399409259196143191337566470144.0000, y 절편 b = -10519765911521304251750928025963213702972921263895848831784790868020420816352755498870827917922506491133905119789873107661637482945492578286839006949365706070607042787496359609454095347851209095910300290048967487641206859350360533300452628559688623080677727885158957598562970022096928768.0000\n",
            "step: 500, cost = nan, 기울기 a = nan, y 절편 b = nan\n",
            "step: 600, cost = nan, 기울기 a = nan, y 절편 b = nan\n",
            "step: 700, cost = nan, 기울기 a = nan, y 절편 b = nan\n",
            "step: 800, cost = nan, 기울기 a = nan, y 절편 b = nan\n",
            "step: 900, cost = nan, 기울기 a = nan, y 절편 b = nan\n",
            "step: 1000, cost = nan, 기울기 a = nan, y 절편 b = nan\n",
            "step: 1100, cost = nan, 기울기 a = nan, y 절편 b = nan\n",
            "step: 1200, cost = nan, 기울기 a = nan, y 절편 b = nan\n",
            "step: 1300, cost = nan, 기울기 a = nan, y 절편 b = nan\n",
            "step: 1400, cost = nan, 기울기 a = nan, y 절편 b = nan\n",
            "step: 1500, cost = nan, 기울기 a = nan, y 절편 b = nan\n",
            "step: 1600, cost = nan, 기울기 a = nan, y 절편 b = nan\n",
            "step: 1700, cost = nan, 기울기 a = nan, y 절편 b = nan\n",
            "step: 1800, cost = nan, 기울기 a = nan, y 절편 b = nan\n",
            "step: 1900, cost = nan, 기울기 a = nan, y 절편 b = nan\n",
            "step: 2000, cost = nan, 기울기 a = nan, y 절편 b = nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i0YNMBs5EyZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "4904f24d-47b9-4c50-e286-ddc0bb4c3bc5"
      },
      "source": [
        "#linear_regression02.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# input data\n",
        "x_train = [ 1, 2, 3]\n",
        "y_train = [10, 20, 30]\n",
        "\n",
        "# tf.random_normal([1]) : 난수 1개 발생\n",
        "W = tf.Variable(tf.random_normal([1]), name='weight') # 기울기, 가중치\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias') # 절편\n",
        "\n",
        "# Our hypothesis XW+b\n",
        "hypothesis = x_train * W + b\n",
        "\n",
        "# cost/loss function\n",
        "# square 함수는 제곱의 값\n",
        "# reduce_mean 함수는 평균\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
        "\n",
        "# Launch the graph in a session.\n",
        "sess = tf.Session()\n",
        "\n",
        "# Initializes global variables in the graph.\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# Minimize\n",
        "# GradientDescentOptimizer 함수는 경사하강법을 구현한 함수임\n",
        "# 경사는 코스트를 가중치로 미분한 값\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
        "\n",
        "# minimize 함수는 최소화핚 결과를 반홖함\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "# Fit the line\n",
        "for step in range(2001):\n",
        "  # 2000번 학습\n",
        "  sess.run(train)\n",
        "  if step % 100 == 0:\n",
        "    print(step, 'cost=',sess.run(cost), 'weight=',sess.run(W), 'bias=',sess.run(b))\n",
        "\n",
        "# Learns best fit W:[ 1.], b:[ 0.]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 cost= 421.91605 weight= [0.9847863] bias= [-1.145946]\n",
            "100 cost= 46.000458 weight= [6.37123] bias= [1.1565725]\n",
            "200 cost= 5.586846 weight= [8.152671] bias= [1.874834]\n",
            "300 cost= 1.2152419 weight= [8.752172] bias= [2.0747135]\n",
            "400 cost= 0.7167538 weight= [8.963921] bias= [2.1055367]\n",
            "500 cost= 0.63568443 weight= [9.048221] bias= [2.0817745]\n",
            "600 cost= 0.6007454 weight= [9.090394] bias= [2.040926]\n",
            "700 cost= 0.5719946 weight= [9.118416] bias= [1.9952626]\n",
            "800 cost= 0.5450831 weight= [9.141457] bias= [1.9487882]\n",
            "900 cost= 0.5194877 weight= [9.16254] bias= [1.9027972]\n",
            "1000 cost= 0.49509847 weight= [9.182661] bias= [1.857695]\n",
            "1100 cost= 0.4718555 weight= [9.202151] bias= [1.8135971]\n",
            "1200 cost= 0.44970337 weight= [9.221128] bias= [1.7705251]\n",
            "1300 cost= 0.42859206 weight= [9.239637] bias= [1.7284696]\n",
            "1400 cost= 0.40847173 weight= [9.257701] bias= [1.6874108]\n",
            "1500 cost= 0.38929534 weight= [9.275337] bias= [1.6473264]\n",
            "1600 cost= 0.3710195 weight= [9.292551] bias= [1.6081939]\n",
            "1700 cost= 0.35360178 weight= [9.309356] bias= [1.5699911]\n",
            "1800 cost= 0.3370017 weight= [9.325763] bias= [1.5326959]\n",
            "1900 cost= 0.3211803 weight= [9.341779] bias= [1.4962864]\n",
            "2000 cost= 0.30610284 weight= [9.357414] bias= [1.4607419]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U45O7DIw5E0n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e20540ba-3b26-4db9-afe8-ec6077be5baa"
      },
      "source": [
        "#linear_regression_feed.py (linear regression을 placeholder로 구현)\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Try to find values for W and b to compute y_data = W * x_data + b\n",
        "# We know that W should be 1 and b should be 0\n",
        "W = tf.Variable(tf.random_normal([1]), name='weight') # 기울기\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias') # 절편\n",
        "\n",
        "# Now we can use X and Y in place of x_data and y_data\n",
        "X = tf.placeholder(tf.float32, shape=[None]) # placeholder 정의, 여러 개의 데이터를 받는다!\n",
        "Y = tf.placeholder(tf.float32, shape=[None])\n",
        "\n",
        "# Our hypothesis XW+b\n",
        "hypothesis = X * W + b\n",
        "\n",
        "# cost/loss function\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "\n",
        "# Minimize\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "# Launch the graph in a session.\n",
        "sess = tf.Session()\n",
        "\n",
        "# Initializes global variables in the graph.\n",
        "sess.run(tf.global_variables_initializer()) # 모든 변수를 초기화\n",
        "\n",
        "#Fit the line\n",
        "for step in range(2001):\n",
        "  # 2000번 학습\n",
        "  cost_val, W_val, b_val, _ = sess.run([cost, W, b, train],feed_dict={X: [1, 2, 3], Y: [1, 2, 3]})\n",
        "  if step % 20 == 0:\n",
        "    # print(step, cost_val, W_val, b_val)\n",
        "    print(step, 'cost=',cost_val, 'weight=',W_val, 'bias=',b_val)\n",
        "# Learns best fit W:[ 1.], b:[ 0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 cost= 4.075312 weight= [0.6445323] bias= [2.5236394]\n",
            "20 cost= 0.6904714 weight= [0.11884148] bias= [2.1664364]\n",
            "40 cost= 0.5997994 weight= [0.10860607] bias= [2.0419028]\n",
            "60 cost= 0.54449993 weight= [0.14558043] bias= [1.9437777]\n",
            "80 cost= 0.49452147 weight= [0.18526667] bias= [1.8522214]\n",
            "100 cost= 0.4491321 weight= [0.22351158] bias= [1.7651541]\n",
            "120 cost= 0.407909 weight= [0.25999942] bias= [1.6821964]\n",
            "140 cost= 0.37046924 weight= [0.2947764] bias= [1.6031389]\n",
            "160 cost= 0.33646607 weight= [0.32791936] bias= [1.5277972]\n",
            "180 cost= 0.305584 weight= [0.3595046] bias= [1.4559965]\n",
            "200 cost= 0.27753627 weight= [0.38960558] bias= [1.3875699]\n",
            "220 cost= 0.2520628 weight= [0.41829193] bias= [1.3223592]\n",
            "240 cost= 0.22892755 weight= [0.44563004] bias= [1.2602131]\n",
            "260 cost= 0.20791559 weight= [0.4716834] bias= [1.2009877]\n",
            "280 cost= 0.18883222 weight= [0.49651238] bias= [1.1445457]\n",
            "300 cost= 0.17150056 weight= [0.5201743] bias= [1.0907563]\n",
            "320 cost= 0.15575953 weight= [0.5427245] bias= [1.0394948]\n",
            "340 cost= 0.14146332 weight= [0.5642147] bias= [0.9906424]\n",
            "360 cost= 0.12847923 weight= [0.584695] bias= [0.9440859]\n",
            "380 cost= 0.11668691 weight= [0.60421276] bias= [0.8997174]\n",
            "400 cost= 0.10597691 weight= [0.6228134] bias= [0.8574339]\n",
            "420 cost= 0.09624991 weight= [0.6405397] bias= [0.81713766]\n",
            "440 cost= 0.08741572 weight= [0.6574331] bias= [0.7787352]\n",
            "460 cost= 0.07939236 weight= [0.6735324] bias= [0.7421375]\n",
            "480 cost= 0.07210544 weight= [0.6888752] bias= [0.70725983]\n",
            "500 cost= 0.06548729 weight= [0.7034969] bias= [0.67402124]\n",
            "520 cost= 0.05947663 weight= [0.7174314] bias= [0.64234465]\n",
            "540 cost= 0.05401763 weight= [0.7307111] bias= [0.6121568]\n",
            "560 cost= 0.04905964 weight= [0.7433667] bias= [0.58338773]\n",
            "580 cost= 0.044556763 weight= [0.75542754] bias= [0.5559707]\n",
            "600 cost= 0.040467184 weight= [0.7669215] bias= [0.52984214]\n",
            "620 cost= 0.036752917 weight= [0.77787536] bias= [0.50494146]\n",
            "640 cost= 0.03337962 weight= [0.7883144] bias= [0.48121113]\n",
            "660 cost= 0.030315904 weight= [0.7982629] bias= [0.458596]\n",
            "680 cost= 0.027533373 weight= [0.80774385] bias= [0.43704364]\n",
            "700 cost= 0.025006263 weight= [0.8167791] bias= [0.4165042]\n",
            "720 cost= 0.022711104 weight= [0.82538974] bias= [0.39693004]\n",
            "740 cost= 0.020626562 weight= [0.8335958] bias= [0.37827584]\n",
            "760 cost= 0.01873338 weight= [0.8414162] bias= [0.36049828]\n",
            "780 cost= 0.017013961 weight= [0.848869] bias= [0.34355617]\n",
            "800 cost= 0.015452358 weight= [0.85597163] bias= [0.32741034]\n",
            "820 cost= 0.014034064 weight= [0.8627404] bias= [0.31202322]\n",
            "840 cost= 0.012745972 weight= [0.8691911] bias= [0.2973593]\n",
            "860 cost= 0.01157609 weight= [0.8753387] bias= [0.28338447]\n",
            "880 cost= 0.010513586 weight= [0.8811974] bias= [0.27006638]\n",
            "900 cost= 0.009548608 weight= [0.88678056] bias= [0.25737426]\n",
            "920 cost= 0.008672192 weight= [0.8921015] bias= [0.24527858]\n",
            "940 cost= 0.007876232 weight= [0.8971723] bias= [0.23375136]\n",
            "960 cost= 0.007153315 weight= [0.90200484] bias= [0.22276595]\n",
            "980 cost= 0.0064967643 weight= [0.9066103] bias= [0.21229681]\n",
            "1000 cost= 0.005900454 weight= [0.91099924] bias= [0.20231964]\n",
            "1020 cost= 0.005358895 weight= [0.915182] bias= [0.19281133]\n",
            "1040 cost= 0.0048670373 weight= [0.9191682] bias= [0.1837499]\n",
            "1060 cost= 0.004420312 weight= [0.9229669] bias= [0.1751143]\n",
            "1080 cost= 0.0040146005 weight= [0.92658716] bias= [0.16688459]\n",
            "1100 cost= 0.0036461242 weight= [0.9300373] bias= [0.15904161]\n",
            "1120 cost= 0.0033114615 weight= [0.93332535] bias= [0.15156722]\n",
            "1140 cost= 0.00300753 weight= [0.93645877] bias= [0.14444412]\n",
            "1160 cost= 0.0027314825 weight= [0.9394451] bias= [0.13765575]\n",
            "1180 cost= 0.002480773 weight= [0.9422909] bias= [0.1311864]\n",
            "1200 cost= 0.002253079 weight= [0.945003] bias= [0.12502111]\n",
            "1220 cost= 0.0020462833 weight= [0.94758767] bias= [0.11914557]\n",
            "1240 cost= 0.001858465 weight= [0.95005095] bias= [0.11354613]\n",
            "1260 cost= 0.0016878844 weight= [0.9523983] bias= [0.10820983]\n",
            "1280 cost= 0.0015329669 weight= [0.9546354] bias= [0.10312437]\n",
            "1300 cost= 0.0013922617 weight= [0.9567674] bias= [0.09827793]\n",
            "1320 cost= 0.0012644761 weight= [0.9587992] bias= [0.09365924]\n",
            "1340 cost= 0.0011484189 weight= [0.9607355] bias= [0.08925754]\n",
            "1360 cost= 0.0010430083 weight= [0.96258074] bias= [0.08506273]\n",
            "1380 cost= 0.0009472825 weight= [0.9643393] bias= [0.08106511]\n",
            "1400 cost= 0.0008603365 weight= [0.9660152] bias= [0.07725537]\n",
            "1420 cost= 0.00078136945 weight= [0.9676124] bias= [0.07362466]\n",
            "1440 cost= 0.000709649 weight= [0.96913445] bias= [0.07016458]\n",
            "1460 cost= 0.0006445203 weight= [0.97058505] bias= [0.06686711]\n",
            "1480 cost= 0.0005853626 weight= [0.97196746] bias= [0.06372459]\n",
            "1500 cost= 0.00053163344 weight= [0.9732849] bias= [0.06072977]\n",
            "1520 cost= 0.00048283953 weight= [0.97454035] bias= [0.0578757]\n",
            "1540 cost= 0.0004385231 weight= [0.97573686] bias= [0.05515577]\n",
            "1560 cost= 0.00039827265 weight= [0.97687715] bias= [0.05256365]\n",
            "1580 cost= 0.00036171786 weight= [0.97796386] bias= [0.05009335]\n",
            "1600 cost= 0.00032852066 weight= [0.9789994] bias= [0.04773916]\n",
            "1620 cost= 0.00029836694 weight= [0.9799864] bias= [0.04549562]\n",
            "1640 cost= 0.00027097928 weight= [0.980927] bias= [0.04335749]\n",
            "1660 cost= 0.0002461091 weight= [0.9818233] bias= [0.04131985]\n",
            "1680 cost= 0.00022352056 weight= [0.9826776] bias= [0.03937796]\n",
            "1700 cost= 0.00020300236 weight= [0.9834917] bias= [0.0375273]\n",
            "1720 cost= 0.00018437183 weight= [0.98426753] bias= [0.03576364]\n",
            "1740 cost= 0.00016744836 weight= [0.9850069] bias= [0.03408289]\n",
            "1760 cost= 0.00015207854 weight= [0.9857115] bias= [0.0324811]\n",
            "1780 cost= 0.00013812074 weight= [0.986383] bias= [0.0309546]\n",
            "1800 cost= 0.0001254434 weight= [0.987023] bias= [0.02949986]\n",
            "1820 cost= 0.00011392969 weight= [0.9876328] bias= [0.02811347]\n",
            "1840 cost= 0.000103472674 weight= [0.988214] bias= [0.02679226]\n",
            "1860 cost= 9.3977396e-05 weight= [0.98876786] bias= [0.02553315]\n",
            "1880 cost= 8.535131e-05 weight= [0.9892958] bias= [0.02433321]\n",
            "1900 cost= 7.751755e-05 weight= [0.9897988] bias= [0.02318964]\n",
            "1920 cost= 7.040148e-05 weight= [0.99027836] bias= [0.0220998]\n",
            "1940 cost= 6.394018e-05 weight= [0.9907352] bias= [0.02106115]\n",
            "1960 cost= 5.8071775e-05 weight= [0.9911706] bias= [0.02007134]\n",
            "1980 cost= 5.274147e-05 weight= [0.99158555] bias= [0.01912806]\n",
            "2000 cost= 4.7900587e-05 weight= [0.99198097] bias= [0.01822912]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aklGVBQrjGgM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "d291abe1-c731-4a4e-aaca-002ffea52a81"
      },
      "source": [
        "#multi_linear_regression.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# x1, x2, y의 데이터 값\n",
        "x1 = [2, 4, 6, 8] # 공부핚 시갂\n",
        "x2 = [0, 4, 2, 3] # 과외 수업 횟수\n",
        "y_data = [81,93,91,97] # 성적\n",
        "\n",
        "# 기울기 a와 y젃편 b의 값을 임의로 정함.\n",
        "# 단 기울기의 범위는 0-10 사이, y 젃편은 0-100사이에서 변하게 함\n",
        "a1 = tf.Variable(tf.random_uniform([1], 0, 10, dtype=tf.float64, seed=0))\n",
        "a2 = tf.Variable(tf.random_uniform([1], 0, 10, dtype=tf.float64, seed=0))\n",
        "b = tf.Variable(tf.random_uniform([1], 0, 100, dtype=tf.float64, seed=0))\n",
        "\n",
        "# 새로운 방정식\n",
        "y = a1 * x1 + a2 * x2 + b\n",
        "\n",
        "# 텐서플로 RMSE 함수(비용 함수)\n",
        "rmse = tf.sqrt(tf.reduce_mean(tf.square( y - y_data )))\n",
        "\n",
        "# 학습률\n",
        "learning_rate = 0.1\n",
        "\n",
        "# 경사 하강법으로 RMSE 값(비용)을 최소로 하는 값 찾기\n",
        "gradient_decent = tf.train.GradientDescentOptimizer(learning_rate).minimize(rmse)\n",
        "\n",
        "# 학습이 진행되는 부분\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  # 모든 변수 초기화\n",
        "  for step in range(2001):\n",
        "    # 2000번 학습\n",
        "    sess.run(gradient_decent)\n",
        "    if step % 100 == 0:\n",
        "       print(\"Epoch: %.f, RMSE = %.4f, 기울기 a1 = %.4f, 기울기 a2 = %.4f, y절편 b = %.4f\"%(step, sess.run(rmse), sess.run(a1), sess.run(a2), sess.run(b)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, RMSE = 49.1842, 기울기 a1 = 7.5270, 기울기 a2 = 7.8160, y절편 b = 80.5980\n",
            "Epoch: 100, RMSE = 1.8368, 기울기 a1 = 1.1306, 기울기 a2 = 2.1316, y절편 b = 78.5119\n",
            "Epoch: 200, RMSE = 1.8370, 기울기 a1 = 1.1879, 기울기 a2 = 2.1487, y절편 b = 78.1057\n",
            "Epoch: 300, RMSE = 1.8370, 기울기 a1 = 1.2122, 기울기 a2 = 2.1571, y절편 b = 77.9352\n",
            "Epoch: 400, RMSE = 1.8370, 기울기 a1 = 1.2226, 기울기 a2 = 2.1607, y절편 b = 77.8636\n",
            "Epoch: 500, RMSE = 1.8370, 기울기 a1 = 1.2269, 기울기 a2 = 2.1622, y절편 b = 77.8335\n",
            "Epoch: 600, RMSE = 1.8370, 기울기 a1 = 1.2288, 기울기 a2 = 2.1628, y절편 b = 77.8208\n",
            "Epoch: 700, RMSE = 1.8370, 기울기 a1 = 1.2295, 기울기 a2 = 2.1631, y절편 b = 77.8155\n",
            "Epoch: 800, RMSE = 1.8370, 기울기 a1 = 1.2299, 기울기 a2 = 2.1632, y절편 b = 77.8133\n",
            "Epoch: 900, RMSE = 1.8370, 기울기 a1 = 1.2300, 기울기 a2 = 2.1632, y절편 b = 77.8124\n",
            "Epoch: 1000, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y절편 b = 77.8120\n",
            "Epoch: 1100, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y절편 b = 77.8118\n",
            "Epoch: 1200, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y절편 b = 77.8117\n",
            "Epoch: 1300, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y절편 b = 77.8117\n",
            "Epoch: 1400, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y절편 b = 77.8117\n",
            "Epoch: 1500, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y절편 b = 77.8117\n",
            "Epoch: 1600, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y절편 b = 77.8117\n",
            "Epoch: 1700, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y절편 b = 77.8117\n",
            "Epoch: 1800, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y절편 b = 77.8117\n",
            "Epoch: 1900, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y절편 b = 77.8117\n",
            "Epoch: 2000, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y절편 b = 77.8117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLixB3aojGiT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c83336fc-1938-4e4b-ac7b-61a2efeb8326"
      },
      "source": [
        "#multi_linear_regression02.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# 학습 데이터\n",
        "x1_data = [73., 93., 89., 96., 73.] # quiz1\n",
        "x2_data = [80., 88., 91., 98., 66.] # quiz2\n",
        "x3_data = [75., 93., 90., 100., 70.] # midterm\n",
        "y_data = [152., 185., 180., 196., 142.] # final\n",
        "\n",
        "# placeholders for a tensor that will be always fed.\n",
        "x1 = tf.placeholder(tf.float32) # placeholder 정의\n",
        "x2 = tf.placeholder(tf.float32)\n",
        "x3 = tf.placeholder(tf.float32)\n",
        "Y = tf.placeholder(tf.float32)\n",
        "w1 = tf.Variable(tf.random_normal([1]), name='weight1')\n",
        "w2 = tf.Variable(tf.random_normal([1]), name='weight2')\n",
        "w3 = tf.Variable(tf.random_normal([1]), name='weight3')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "hypothesis = x1 * w1 + x2 * w2 + x3 * w3 + b\n",
        "\n",
        "# cost/loss function : 비용함수\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "\n",
        "# Minimize. Need a very small learning rate for this data set\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.00003)\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "# Launch the graph in a session.\n",
        "sess = tf.Session() # 세션 설정\n",
        "\n",
        "# Initializes global variables in the graph.\n",
        "# 모든 변수 초기화\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# 10000번 학습\n",
        "for step in range(10001):\n",
        "  cost_val, hy_val, _= sess.run([cost, hypothesis, train],\n",
        "                                feed_dict={x1: x1_data, x2: x2_data, x3: x3_data, Y: y_data})\n",
        "  if step % 100 == 0:\n",
        "    print(\"step =\", step, \"Cost: \", cost_val, \"Prediction:\", hy_val)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step = 0 Cost:  86586.73 Prediction: [ -98.351746 -136.74861  -124.9335   -137.62994  -107.88419 ]\n",
            "step = 100 Cost:  53.890034 Prediction: [161.7572  177.56206 183.88898 198.76552 132.19429]\n",
            "step = 200 Cost:  45.84134 Prediction: [160.93736 178.12448 183.63828 198.58032 132.93565]\n",
            "step = 300 Cost:  39.00068 Prediction: [160.18167 178.64293 183.40721 198.40935 133.61923]\n",
            "step = 400 Cost:  33.18672 Prediction: [159.48512 179.12082 183.19426 198.25153 134.24957]\n",
            "step = 500 Cost:  28.245382 Prediction: [158.84312 179.56133 182.998   198.10583 134.83084]\n",
            "step = 600 Cost:  24.045527 Prediction: [158.25133 179.96736 182.81714 197.97128 135.36684]\n",
            "step = 700 Cost:  20.475922 Prediction: [157.70589 180.34166 182.65047 197.84706 135.86113]\n",
            "step = 800 Cost:  17.442074 Prediction: [157.2032  180.68669 182.49692 197.73235 136.31697]\n",
            "step = 900 Cost:  14.863525 Prediction: [156.73987 181.00467 182.3554  197.62637 136.73734]\n",
            "step = 1000 Cost:  12.671856 Prediction: [156.31282 181.29778 182.22499 197.52846 137.125  ]\n",
            "step = 1100 Cost:  10.808989 Prediction: [155.91924 181.56795 182.10486 197.438   137.48256]\n",
            "step = 1200 Cost:  9.225717 Prediction: [155.55653 181.81697 181.99417 197.35446 137.81232]\n",
            "step = 1300 Cost:  7.8799477 Prediction: [155.22226 182.04648 181.89218 197.27719 138.11646]\n",
            "step = 1400 Cost:  6.7360535 Prediction: [154.91418 182.258   181.7982  197.20576 138.39697]\n",
            "step = 1500 Cost:  5.7637544 Prediction: [154.63033 182.45299 181.71167 197.13976 138.65575]\n",
            "step = 1600 Cost:  4.9372797 Prediction: [154.3687  182.63266 181.63191 197.07867 138.89441]\n",
            "step = 1700 Cost:  4.2348037 Prediction: [154.12766 182.79826 181.5585  197.02223 139.1146 ]\n",
            "step = 1800 Cost:  3.6376164 Prediction: [153.90552 182.95088 181.49084 196.96994 139.31769]\n",
            "step = 1900 Cost:  3.129962 Prediction: [153.70084 183.09152 181.42853 196.92157 139.50507]\n",
            "step = 2000 Cost:  2.6984372 Prediction: [153.51227 183.22113 181.37119 196.8768  139.67795]\n",
            "step = 2100 Cost:  2.331538 Prediction: [153.33849 183.34056 181.31833 196.8353  139.83743]\n",
            "step = 2200 Cost:  2.0196278 Prediction: [153.17842 183.45064 181.26968 196.79692 139.98462]\n",
            "step = 2300 Cost:  1.7544453 Prediction: [153.03096 183.55205 181.22491 196.76132 140.12044]\n",
            "step = 2400 Cost:  1.5289482 Prediction: [152.89511 183.64551 181.18369 196.72832 140.24577]\n",
            "step = 2500 Cost:  1.3372264 Prediction: [152.76999 183.73163 181.14575 196.69774 140.36143]\n",
            "step = 2600 Cost:  1.1741797 Prediction: [152.65472 183.81094 181.11082 196.66936 140.4682 ]\n",
            "step = 2700 Cost:  1.0355043 Prediction: [152.54857 183.88403 181.07869 196.643   140.56676]\n",
            "step = 2800 Cost:  0.91755885 Prediction: [152.45079 183.95135 181.0491  196.61853 140.65771]\n",
            "step = 2900 Cost:  0.81722224 Prediction: [152.36076 184.0134  181.02191 196.59581 140.74171]\n",
            "step = 3000 Cost:  0.7318556 Prediction: [152.27788 184.07056 180.9969  196.57469 140.81927]\n",
            "step = 3100 Cost:  0.6592206 Prediction: [152.20157 184.12318 180.9739  196.55504 140.8909 ]\n",
            "step = 3200 Cost:  0.59740096 Prediction: [152.13127 184.17162 180.95273 196.53673 140.95703]\n",
            "step = 3300 Cost:  0.544766 Prediction: [152.06659 184.21628 180.93327 196.51971 141.01813]\n",
            "step = 3400 Cost:  0.49994785 Prediction: [152.00706 184.2574  180.9154  196.50386 141.07458]\n",
            "step = 3500 Cost:  0.46177578 Prediction: [151.95229 184.29526 180.89899 196.48906 141.12674]\n",
            "step = 3600 Cost:  0.42925614 Prediction: [151.90187 184.33011 180.88391 196.47527 141.17494]\n",
            "step = 3700 Cost:  0.4015286 Prediction: [151.85548 184.3622  180.87004 196.46237 141.21948]\n",
            "step = 3800 Cost:  0.3778953 Prediction: [151.8128  184.39171 180.85733 196.45032 141.26067]\n",
            "step = 3900 Cost:  0.3577128 Prediction: [151.77359 184.41893 180.84567 196.43906 141.29877]\n",
            "step = 4000 Cost:  0.34049517 Prediction: [151.73749 184.44394 180.83496 196.42851 141.33398]\n",
            "step = 4100 Cost:  0.32577866 Prediction: [151.70432 184.46696 180.82513 196.41862 141.36656]\n",
            "step = 4200 Cost:  0.31319517 Prediction: [151.67383 184.48814 180.81613 196.40935 141.39671]\n",
            "step = 4300 Cost:  0.30244178 Prediction: [151.64581 184.50763 180.80792 196.40067 141.42462]\n",
            "step = 4400 Cost:  0.29320806 Prediction: [151.62006 184.52556 180.80035 196.39246 141.45045]\n",
            "step = 4500 Cost:  0.28529406 Prediction: [151.59642 184.54205 180.79346 196.38477 141.4744 ]\n",
            "step = 4600 Cost:  0.27848962 Prediction: [151.5747  184.5572  180.78712 196.37749 141.49657]\n",
            "step = 4700 Cost:  0.27265164 Prediction: [151.55478 184.57112 180.78136 196.37067 141.5171 ]\n",
            "step = 4800 Cost:  0.26761094 Prediction: [151.5365  184.58394 180.7761  196.36421 141.53616]\n",
            "step = 4900 Cost:  0.2632626 Prediction: [151.51974 184.59569 180.77129 196.35812 141.55383]\n",
            "step = 5000 Cost:  0.25949937 Prediction: [151.50438 184.60652 180.76692 196.35237 141.57024]\n",
            "step = 5100 Cost:  0.25622833 Prediction: [151.49028 184.61641 180.7629  196.34686 141.58543]\n",
            "step = 5200 Cost:  0.25339952 Prediction: [151.47736 184.6255  180.75928 196.34167 141.59956]\n",
            "step = 5300 Cost:  0.2509105 Prediction: [151.46556 184.63387 180.75597 196.33673 141.6127 ]\n",
            "step = 5400 Cost:  0.24873872 Prediction: [151.45476 184.64153 180.75298 196.33202 141.62491]\n",
            "step = 5500 Cost:  0.24682856 Prediction: [151.44489 184.64856 180.75026 196.32756 141.63628]\n",
            "step = 5600 Cost:  0.24515316 Prediction: [151.43584 184.65498 180.7478  196.32329 141.64684]\n",
            "step = 5700 Cost:  0.24366057 Prediction: [151.42763 184.66092 180.74562 196.31924 141.65671]\n",
            "step = 5800 Cost:  0.24232483 Prediction: [151.4201  184.66632 180.74359 196.31532 141.66586]\n",
            "step = 5900 Cost:  0.2411438 Prediction: [151.41325 184.67125 180.7418  196.3116  141.67442]\n",
            "step = 6000 Cost:  0.24007428 Prediction: [151.40701 184.67575 180.74019 196.308   141.68242]\n",
            "step = 6100 Cost:  0.23910508 Prediction: [151.40135 184.6799  180.73875 196.30457 141.6899 ]\n",
            "step = 6200 Cost:  0.23823532 Prediction: [151.39621 184.68367 180.73749 196.30127 141.69685]\n",
            "step = 6300 Cost:  0.23743233 Prediction: [151.39154 184.6871  180.73634 196.29808 141.70338]\n",
            "step = 6400 Cost:  0.236696 Prediction: [151.38734 184.69025 180.73537 196.29503 141.70952]\n",
            "step = 6500 Cost:  0.23601401 Prediction: [151.38353 184.6931  180.73448 196.29207 141.71526]\n",
            "step = 6600 Cost:  0.23538616 Prediction: [151.38008 184.6957  180.73372 196.2892  141.72063]\n",
            "step = 6700 Cost:  0.23479278 Prediction: [151.37697 184.69804 180.73303 196.28639 141.72566]\n",
            "step = 6800 Cost:  0.23423526 Prediction: [151.37419 184.70016 180.73245 196.28368 141.73041]\n",
            "step = 6900 Cost:  0.23372214 Prediction: [151.3717  184.70212 180.732   196.2811  141.73488]\n",
            "step = 7000 Cost:  0.23322363 Prediction: [151.36949 184.70386 180.73158 196.27855 141.73909]\n",
            "step = 7100 Cost:  0.23275682 Prediction: [151.36751 184.70544 180.73126 196.27608 141.74307]\n",
            "step = 7200 Cost:  0.2323139 Prediction: [151.36575 184.70686 180.731   196.27368 141.74681]\n",
            "step = 7300 Cost:  0.23187566 Prediction: [151.36423 184.70813 180.73079 196.27133 141.75038]\n",
            "step = 7400 Cost:  0.23146817 Prediction: [151.36288 184.70927 180.73067 196.26904 141.75374]\n",
            "step = 7500 Cost:  0.23106149 Prediction: [151.36172 184.71031 180.73056 196.26682 141.75693]\n",
            "step = 7600 Cost:  0.23068576 Prediction: [151.3607  184.71121 180.73053 196.26463 141.75995]\n",
            "step = 7700 Cost:  0.23031726 Prediction: [151.35986 184.71202 180.73056 196.26251 141.76283]\n",
            "step = 7800 Cost:  0.22994845 Prediction: [151.35915 184.71272 180.73058 196.26042 141.76556]\n",
            "step = 7900 Cost:  0.22960588 Prediction: [151.35854 184.71335 180.73067 196.25838 141.76816]\n",
            "step = 8000 Cost:  0.22926152 Prediction: [151.35805 184.71388 180.73077 196.25635 141.77066]\n",
            "step = 8100 Cost:  0.22892526 Prediction: [151.35768 184.71434 180.73091 196.2544  141.77306]\n",
            "step = 8200 Cost:  0.22859815 Prediction: [151.35742 184.71475 180.7311  196.25247 141.77533]\n",
            "step = 8300 Cost:  0.22828165 Prediction: [151.35722 184.7151  180.7313  196.25058 141.77751]\n",
            "step = 8400 Cost:  0.22796795 Prediction: [151.35712 184.71536 180.7315  196.24869 141.7796 ]\n",
            "step = 8500 Cost:  0.2276549 Prediction: [151.35712 184.7156  180.73175 196.24687 141.78162]\n",
            "step = 8600 Cost:  0.2273554 Prediction: [151.35715 184.71577 180.73201 196.24506 141.78357]\n",
            "step = 8700 Cost:  0.22706187 Prediction: [151.35724 184.71591 180.73228 196.24327 141.78543]\n",
            "step = 8800 Cost:  0.22677001 Prediction: [151.35742 184.71599 180.73259 196.2415  141.78723]\n",
            "step = 8900 Cost:  0.22649363 Prediction: [151.3576  184.71603 180.7329  196.23976 141.78896]\n",
            "step = 9000 Cost:  0.22620337 Prediction: [151.35788 184.71606 180.7332  196.23805 141.79065]\n",
            "step = 9100 Cost:  0.22593197 Prediction: [151.35817 184.71606 180.73354 196.23637 141.79228]\n",
            "step = 9200 Cost:  0.22566104 Prediction: [151.35854 184.71603 180.7339  196.23473 141.79388]\n",
            "step = 9300 Cost:  0.2253913 Prediction: [151.35889 184.71594 180.7342  196.23306 141.7954 ]\n",
            "step = 9400 Cost:  0.22513184 Prediction: [151.35931 184.71588 180.7346  196.23145 141.7969 ]\n",
            "step = 9500 Cost:  0.22487096 Prediction: [151.35974 184.71576 180.73494 196.22984 141.79834]\n",
            "step = 9600 Cost:  0.22461724 Prediction: [151.36023 184.71564 180.73534 196.22827 141.79977]\n",
            "step = 9700 Cost:  0.22436793 Prediction: [151.36072 184.71548 180.73572 196.22668 141.80115]\n",
            "step = 9800 Cost:  0.2241153 Prediction: [151.36124 184.71532 180.73608 196.22514 141.8025 ]\n",
            "step = 9900 Cost:  0.22387967 Prediction: [151.36174 184.71512 180.73647 196.22357 141.80379]\n",
            "step = 10000 Cost:  0.22364192 Prediction: [151.3623  184.71494 180.73688 196.22208 141.8051 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSRSsdcT9lHY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c86384f8-2389-4704-ebaf-7fbb4f7991c5"
      },
      "source": [
        "#logistic_regression01.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# 학습 데이터\n",
        "# [ 공부시간, 과외받은 횟수 ]\n",
        "x_data = [[1, 2],\n",
        "          [2, 3],\n",
        "          [3, 1],\n",
        "          [4, 3],\n",
        "          [5, 3],\n",
        "          [6, 2]]\n",
        "# 1:합격, 0:불합격\n",
        "y_data = [[0], \n",
        "          [0],\n",
        "          [0],\n",
        "          [1],\n",
        "          [1],\n",
        "          [1]]\n",
        "\n",
        "# placeholders for a tensor that will be always fed.\n",
        "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
        "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W))) : 가설, 모델\n",
        "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
        "\n",
        "# cost/loss function : 비용함수\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "# Accuracy computation : 정확도 계산\n",
        "# True if hypothesis>0.5 else False\n",
        "# tf.cast()함수는 True면 1, False면 0을 리턴함\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32) # predicted = 1 or 0\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "\n",
        "# Launch graph\n",
        "with tf.Session() as sess:\n",
        "   # Initialize TensorFlow variables : 모든 변수 초기화\n",
        "   sess.run(tf.global_variables_initializer())\n",
        "   # 10000번 학습\n",
        "   for step in range(10001):\n",
        "     cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
        "     # 200번 마다 출력\n",
        "     if step % 200 == 0:\n",
        "       print(\"step = \", step, \"cost =\", cost_val)\n",
        "   # Accuracy report\n",
        "   h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
        "   print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step =  0 cost = 0.5293753\n",
            "step =  200 cost = 0.43999457\n",
            "step =  400 cost = 0.4140042\n",
            "step =  600 cost = 0.39517543\n",
            "step =  800 cost = 0.3798664\n",
            "step =  1000 cost = 0.36653242\n",
            "step =  1200 cost = 0.3544561\n",
            "step =  1400 cost = 0.34327814\n",
            "step =  1600 cost = 0.3328046\n",
            "step =  1800 cost = 0.32292297\n",
            "step =  2000 cost = 0.313562\n",
            "step =  2200 cost = 0.30467254\n",
            "step =  2400 cost = 0.29621747\n",
            "step =  2600 cost = 0.2881666\n",
            "step =  2800 cost = 0.28049418\n",
            "step =  3000 cost = 0.27317718\n",
            "step =  3200 cost = 0.2661946\n",
            "step =  3400 cost = 0.25952718\n",
            "step =  3600 cost = 0.25315675\n",
            "step =  3800 cost = 0.24706651\n",
            "step =  4000 cost = 0.24124068\n",
            "step =  4200 cost = 0.23566441\n",
            "step =  4400 cost = 0.23032367\n",
            "step =  4600 cost = 0.2252056\n",
            "step =  4800 cost = 0.22029774\n",
            "step =  5000 cost = 0.2155887\n",
            "step =  5200 cost = 0.21106769\n",
            "step =  5400 cost = 0.20672464\n",
            "step =  5600 cost = 0.20255004\n",
            "step =  5800 cost = 0.19853501\n",
            "step =  6000 cost = 0.1946712\n",
            "step =  6200 cost = 0.19095081\n",
            "step =  6400 cost = 0.18736643\n",
            "step =  6600 cost = 0.18391125\n",
            "step =  6800 cost = 0.18057872\n",
            "step =  7000 cost = 0.1773629\n",
            "step =  7200 cost = 0.17425795\n",
            "step =  7400 cost = 0.1712585\n",
            "step =  7600 cost = 0.16835962\n",
            "step =  7800 cost = 0.16555648\n",
            "step =  8000 cost = 0.16284455\n",
            "step =  8200 cost = 0.16021971\n",
            "step =  8400 cost = 0.15767798\n",
            "step =  8600 cost = 0.15521552\n",
            "step =  8800 cost = 0.15282886\n",
            "step =  9000 cost = 0.15051457\n",
            "step =  9200 cost = 0.14826964\n",
            "step =  9400 cost = 0.14609097\n",
            "step =  9600 cost = 0.14397573\n",
            "step =  9800 cost = 0.14192142\n",
            "step =  10000 cost = 0.13992535\n",
            "\n",
            "Hypothesis:  [[0.02681956]\n",
            " [0.15316704]\n",
            " [0.2855192 ]\n",
            " [0.79036534]\n",
            " [0.94509494]\n",
            " [0.98203707]] \n",
            "Correct (Y):  [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]] \n",
            "Accuracy:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDd2Hyj19lJl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "df5af564-b1be-4c1c-a044-d8af985a1b21"
      },
      "source": [
        "#logistic_regression02.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 실행할 때마다 같은 결과를 출력하기 위핚 seed 값 설정\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "# 학습 데이터\n",
        "x_data = np.array([[2, 3],[4, 3],[6, 4],[8, 6],[10, 7],[12, 8],[14, 9]])\n",
        "y_data = np.array([0, 0, 0, 1, 1, 1,1]).reshape(7, 1)\n",
        "\n",
        "# 플레이스 홀더 정의\n",
        "X = tf.placeholder(tf.float64, shape=[None, 2])\n",
        "Y = tf.placeholder(tf.float64, shape=[None, 1])\n",
        "\n",
        "# 기울기 a와 bias b의 값을 임의로 정함.\n",
        "a = tf.Variable(tf.random_uniform([2,1], dtype=tf.float64)) # 2행 1열의 난수 발생\n",
        "b = tf.Variable(tf.random_uniform([1], dtype=tf.float64)) # 1개의 난수 발생\n",
        "\n",
        "# y 시그모이드 함수의 방정식을 세움\n",
        "y = tf.sigmoid(tf.matmul(X, a) + b)\n",
        "\n",
        "# 오차를 구하는 함수\n",
        "loss = -tf.reduce_mean(Y * tf.log(y) + (1 - Y) * tf.log(1 - y))\n",
        "\n",
        "# 학습률 값\n",
        "learning_rate=0.1\n",
        "\n",
        "# 경사 하강법으로 오차(비용)를 최소로 하는 값 찾기\n",
        "gradient_decent = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
        "predicted = tf.cast(y > 0.5, dtype=tf.float64) # tf.cast()함수는 True면 1, Flase면 0을 리턴함\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float64))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  # 3000번 학습\n",
        "  for i in range(3001):\n",
        "     a_, b_, loss_, _ = sess.run([a, b, loss, gradient_decent], feed_dict={X: x_data, Y: y_data})\n",
        "     if (i + 1) % 300 == 0:\n",
        "       print(\"step = %d, a1 = %.4f, a2 = %.4f, b = %.4f, loss = %.4f\" % (i + 1, a_[0], a_[1], b_, loss_))\n",
        "  # 공부시간, 개인 과외수, 합격 가능성\n",
        "  new_x = np.array([7, 6]).reshape(1, 2) #[7, 6]은 각각 공부 시갂과 과외 수업수.\n",
        "  new_y = sess.run(y, feed_dict={X: new_x})\n",
        "  print(\"공부 시간: %d, 개인 과외 수: %d\" % (new_x[:,0], new_x[:,1]))\n",
        "  print(\"합격 가능성: %6.2f %%\" % (new_y*100))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step = 300, a1 = 0.8139, a2 = -0.5527, b = -2.4268, loss = 0.2671\n",
            "step = 600, a1 = 0.8157, a2 = -0.2837, b = -3.8951, loss = 0.1915\n",
            "step = 900, a1 = 0.7280, a2 = 0.0405, b = -4.9585, loss = 0.1498\n",
            "step = 1200, a1 = 0.6262, a2 = 0.3444, b = -5.8001, loss = 0.1227\n",
            "step = 1500, a1 = 0.5286, a2 = 0.6158, b = -6.4982, loss = 0.1036\n",
            "step = 1800, a1 = 0.4401, a2 = 0.8554, b = -7.0951, loss = 0.0895\n",
            "step = 2100, a1 = 0.3613, a2 = 1.0672, b = -7.6168, loss = 0.0787\n",
            "step = 2400, a1 = 0.2915, a2 = 1.2553, b = -8.0803, loss = 0.0702\n",
            "step = 2700, a1 = 0.2297, a2 = 1.4234, b = -8.4972, loss = 0.0633\n",
            "step = 3000, a1 = 0.1746, a2 = 1.5747, b = -8.8762, loss = 0.0577\n",
            "공부 시간: 7, 개인 과외 수: 6\n",
            "합격 가능성:  85.75 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2xwmkA-9lLi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "a1438dc7-c243-4568-d48a-b2eb5b572bd6"
      },
      "source": [
        "#bmi.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# 키, 몸무게, 레이블이 적힌 CSV 파일 읽어 들이기\n",
        "csv = pd.read_csv(\"bmi.csv\")\n",
        "\n",
        "# 데이터 정규화\n",
        "csv[\"height\"] = csv[\"height\"] / 200\n",
        "csv[\"weight\"] = csv[\"weight\"] / 100\n",
        "\n",
        "# 레이블을 배열로 변환\n",
        "# thin=(1,0,0) / normal=(0,1,0) / fat=(0,0,1)\n",
        "bclass = {\"thin\": [1,0,0], \"normal\": [0,1,0], \"fat\": [0,0,1]}\n",
        "csv[\"label_pat\"] = csv[\"label\"].apply(lambda x : np.array(bclass[x]))\n",
        "\n",
        "# 플레이스홀더 선언하기\n",
        "x = tf.placeholder(tf.float32, [None, 2]) # 키와 몸무게 데이터 넣기\n",
        "y_ = tf.placeholder(tf.float32, [None, 3]) # 정답 레이블 넣기\n",
        "\n",
        "# 변수 선언하기\n",
        "W = tf.Variable(tf.zeros([2, 3]));\n",
        "b = tf.Variable(tf.zeros([3]));\n",
        "\n",
        "# 소프트맥스 회귀 정의하기\n",
        "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
        "\n",
        "# 비용함수, 경사 하강법 적용\n",
        "cross_entropy = -tf.reduce_sum(y_ * tf.log(y))\n",
        "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
        "train = optimizer.minimize(cross_entropy)\n",
        "\n",
        "# 정답률 구하기 ( tf.argmax(y,1) : y 값 중에서 가장 큰값의 인덱스 번호를 리턴 )\n",
        "predict = tf.equal(tf.argmax(y, 1), tf.argmax(y_,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(predict, tf.float32))\n",
        "\n",
        "# 세션 시작하기\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer()) # 모든 변수 초기화하기\n",
        "\n",
        "# 3500번 학습\n",
        "for step in range(3500):\n",
        "  i = (step * 100) % 14000\n",
        "  rows = csv[1 + i : 1 + i + 100]\n",
        "  x_pat = rows[[\"weight\",\"height\"]]\n",
        "  y_ans = list(rows[\"label_pat\"])\n",
        "  fd = {x: x_pat, y_: y_ans}\n",
        "  sess.run(train, feed_dict=fd)\n",
        "  if step % 500 == 0:\n",
        "    cre = sess.run(cross_entropy, feed_dict=fd)\n",
        "    acc = sess.run(accuracy, feed_dict={x: x_pat, y_: y_ans})\n",
        "  print(\"step = \", step, \"cre = \", cre, \"acc = \", acc)\n",
        "\n",
        "# 최종적인 정답률 구하기\n",
        "acc = sess.run(accuracy, feed_dict={x: x_pat, y_: y_ans})\n",
        "print(\"정답률 =\", acc)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-2d7d12d6318b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 키, 몸무게, 레이블이 적힌 CSV 파일 읽어 들이기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bmi.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 데이터 정규화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'bmi.csv' does not exist: b'bmi.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4QPEPn2X6Hu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "outputId": "9128f2bc-6e7f-45c2-daca-4d227292261c"
      },
      "source": [
        "#softmax_classifier.py\n",
        "\n",
        "import tensorflow as tf\n",
        "# 실행결과가 매번 동일하게 출력 되도록 seed를 설정\n",
        "tf.set_random_seed(777)\n",
        "\n",
        "# 특징 4가지\n",
        "x_data = [[1, 2, 1, 1]\n",
        "          , [2, 1, 3, 2]\n",
        "          , [3, 1, 3, 4]\n",
        "          , [4, 1, 5, 5]\n",
        "          , [1, 7, 5, 5]\n",
        "          , [1, 2, 5, 6]\n",
        "          , [1, 6, 6, 6]\n",
        "          , [1, 7, 7, 7]]\n",
        "\n",
        "# 분류 3가지, 다중 분류\n",
        "y_data = [[0, 0, 1]\n",
        "          , [0, 0, 1]\n",
        "          , [0, 0, 1]\n",
        "          , [0, 1, 0]\n",
        "          , [0, 1, 0]\n",
        "          , [0, 1, 0]\n",
        "          , [1, 0, 0]\n",
        "          , [1, 0, 0]]\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, 4]) # 4열\n",
        "Y = tf.placeholder(\"float\", [None, 3]) # 3열\n",
        "\n",
        "nb_classes = 3\n",
        "\n",
        "# softmax함수에 입력값: 4 , 출력값: nb_classes=3\n",
        "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
        "\n",
        "# tf.nn.softmax computes softmax activations\n",
        "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
        "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
        "\n",
        "# Cross entropy cost/loss\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
        "\n",
        "# 경사하강법을 이용해서 cost가 최소가 되도록 학습\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "# Launch graph\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  # 2000번 학습\n",
        "  for step in range(2001):\n",
        "    sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n",
        "    if step % 200 == 0:\n",
        "      print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
        "  print('--------------')\n",
        "\n",
        "# 경사하강법을 이용해서 cost가 최소가 되도록 학습\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "# Launch graph\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  # 2000번 학습\n",
        "  for step in range(2001):\n",
        "    sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n",
        "    if step % 200 == 0:\n",
        "      print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
        "  print('--------------')\n",
        "  # Testing & One-hot encoding\n",
        "  a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})\n",
        "  print(a, sess.run(tf.arg_max(a, 1)))\n",
        "  print('--------------')\n",
        "  \n",
        "  b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})\n",
        "  print(b, sess.run(tf.arg_max(b, 1))) # arg_max()함수 : one-hot-encoding을 맊들어 주는 함수\n",
        "  print('--------------')\n",
        "  \n",
        "  c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})\n",
        "  print(c, sess.run(tf.arg_max(c, 1)))\n",
        "  print('--------------')\n",
        "  \n",
        "  all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})\n",
        "  print(all, sess.run(tf.arg_max(all, 1)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 2.2075143\n",
            "200 0.53052104\n",
            "400 0.41918337\n",
            "600 0.32158136\n",
            "800 0.25144106\n",
            "1000 0.22609563\n",
            "1200 0.20546351\n",
            "1400 0.18822896\n",
            "1600 0.17360139\n",
            "1800 0.16102777\n",
            "2000 0.15010586\n",
            "--------------\n",
            "0 2.2075143\n",
            "200 0.53052104\n",
            "400 0.41918337\n",
            "600 0.32158136\n",
            "800 0.25144106\n",
            "1000 0.22609563\n",
            "1200 0.20546351\n",
            "1400 0.18822896\n",
            "1600 0.17360139\n",
            "1800 0.16102777\n",
            "2000 0.15010586\n",
            "--------------\n",
            "[[6.9164871e-03 9.9307764e-01 5.9254453e-06]] [1]\n",
            "--------------\n",
            "[[0.83156794 0.15980738 0.0086247 ]] [0]\n",
            "--------------\n",
            "[[5.7785408e-09 2.6351376e-04 9.9973649e-01]] [2]\n",
            "--------------\n",
            "[[6.9164806e-03 9.9307764e-01 5.9254398e-06]\n",
            " [8.3156794e-01 1.5980743e-01 8.6247018e-03]\n",
            " [5.7785412e-09 2.6351379e-04 9.9973649e-01]] [1 0 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5n_a5DoYIk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "--------------\n",
        "[[ 1.38904958e-03 9.98601854e-01 9.06129117e-06]] [1] # index번호 1번이 가장 큰값\n",
        "--------------\n",
        "[[ 0.93119204 0.06290206 0.0059059 ]] [0] # index번호 0번이 가장 큰값\n",
        "--------------\n",
        "[[ 1.27327668e-08 3.34112905e-04 9.99665856e-01]] [2] # index번호 2번이 가장 큰값\n",
        "--------------\n",
        "[[ 1.38904958e-03 9.98601854e-01 9.06129117e-06] # index번호 1번이 가장 큰값\n",
        "[ 9.31192040e-01 6.29020557e-02 5.90589503e-03] # index번호 0번이 가장 큰값\n",
        "[ 1.27327668e-08 3.34112905e-04 9.99665856e-01]] [1 0 2] # index번호 2번이 가장 큰값\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}